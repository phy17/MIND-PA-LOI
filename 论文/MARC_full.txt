                                                                                                                                                                                                              1




                                            MARC: Multipolicy and Risk-aware Contingency
                                                 Planning for Autonomous Driving
                                                                                  Tong Li1 , Lu Zhang1 , Sikang Liu2 , Shaojie Shen1



                                            Abstract—Generating safe and non-conservative behaviors in
                                         dense, dynamic environments remains challenging for automated
                                         vehicles due to the stochastic nature of traffic participants’
                                         behaviors and their implicit interaction with the ego vehicle. This
arXiv:2308.12021v2 [cs.RO] 12 Sep 2023




                                         paper presents a novel planning framework, Multipolicy And
                                         Risk-aware Contingency planning (MARC), that systematically
                                         addresses these challenges by enhancing the multipolicy-based
                                         pipelines from both behavior and motion planning aspects.
                                         Specifically, MARC realizes a critical scenario set that reflects
                                         multiple possible futures conditioned on each semantic-level
                                         ego policy. Then, the generated policy-conditioned scenarios are
                                         further formulated into a tree-structured representation with
                                         a dynamic branchpoint based on the scene-level divergence.
                                         Moreover, to generate diverse driving maneuvers, we introduce
                                         risk-aware contingency planning, a bi-level optimization algo-
                                         rithm that simultaneously considers multiple future scenarios
                                         and user-defined risk tolerance levels. Owing to the more
                                         unified combination of behavior and motion planning layers,
                                         our framework achieves efficient decision-making and human-
                                         like driving maneuvers. Comprehensive experimental results
                                         demonstrate superior performance to other strong baselines in
                                                                                                                     Fig. 1: Illustration of different policies generated by MARC. Policy-conditioned critical
                                         various environments.                                                       scenarios are rendered with different key vehicles. The darkness of trajectories indicates
                                                                                                                     the associated probabilities. Scenario trees are adopted to depict the topology of rendered
                                           Index Terms—Autonomous vehicle navigation, motion and                     scenarios, and trajectory trees with risk-aware maneuvers are generated. The aggressive
                                         path planning, intelligent transportation systems.                          policy shows a siding action considering multi-modal interactions with the surrounding
                                                                                                                     key vehicles, while the conservative policy converges to a smooth deceleration maneuver.


                                                                  I. I NTRODUCTION                                   decision processes in stochastic environments. However, solv-
                                                                                                                     ing such problems becomes computationally intractable as
                                                VER the past decade, autonomous driving has experi-
                                         O      enced remarkable progress. Nonetheless, ensuring safe
                                         and efficient operations in highly interactive environments
                                                                                                                     the size increases. To address this issue, multipolicy-based
                                                                                                                     pipelines [5]–[7] are proposed, which prune the belief trees
                                                                                                                     heavily and decompose the original problem into a limited
                                         remains a formidable challenge. The difficulties of planning                number of closed-loop policy evaluations. These methods
                                         under uncertainty result from imperfect observation and, more               leverage domain-specific semantic-level policies (e.g., lane-
                                         importantly, the inherently multimodal intentions of various                keeping and lane-changing) to approximate the action space
                                         road users that cannot be observed directly. A false estimation             and realize multiple future scenarios for each policy to handle
                                         of the other’s purpose can lead the autonomous vehicle to                   interaction uncertainties. However, these methods are typically
                                         generate overly cautious or hazardous driving behavior, putting             designed to generate the best policy over all possible future
                                         traffic safety at risk.                                                     evolutions, while the subsequent trajectory generation modules
                                            The academic community has extensively explored methods                  only account for a single selected scenario, making the whole
                                         to address the challenges. For interactive decision-making,                 system hard to exploit the multi-modality of decision-making.
                                         existing methods [1]–[4] usually utilize Markov decision                       Another way of interaction handling is generating con-
                                         process (MDP) and partially observable Markov decision                      tingency plans for multiple possible futures in the motion
                                         process (POMDP), mathematically rigorous formulations for                   planning layers. Unlike the aforementioned decision-making
                                                                                                                     algorithms, contingency planning [8, 9] is always formulated
                                           Manuscript received April 28, 2023; accepted August 21, 2023. This
                                         paper was recommended for publication by Editor Hyungpil Moon upon          as a numerical optimization problem, in which the environ-
                                         evaluation of the reviewers’ comments. This work was supported by the       mental uncertainties can be properly handled by optimizing
                                         Hong Kong Ph.D. Fellowship Scheme, The Research Grants Council General      a tree-structured trajectory with each branch accounting for
                                         Research Fund(RGC GRF) project RMGS20EG20, and the HKUST-DJI Joint
                                         Innovation Laboratory.(Corresponding author: Tong Li.)                      a potential hazard. While existing methods [9]–[11] have
                                           1 T. Li, L. Zhang and S. Shen are with the Department of Electronic and
                                                                                                                     demonstrated the ability to handle stochastic interactions with
                                         Computer Engineering, Hong Kong University of Science and Technology,       other road users, their limited ability to handle a small
                                         Hong Kong (e-mail: tlibm@ust.hk; lzhangbz@ust.hk; eeshaojie@ust.hk).
                                           2 S. Liu is with the DJI Technology Company, Ltd., Shenzhen, China (e-    number of surrounding agents may pose challenges for real-
                                         mail: sikang.liu@dji.com).                                                  world applications. Moreover, existing approaches introduce
                                                                                                                                     2



strong assumptions on the upper-stream modules, making them         horizon to enable more flexible decision-making in dynamic
poorly integrated into the existing autonomous driving stack.       scenarios. The EPSILON framework extends forward sim-
   To overcome these limitations, we proposed the MARC              ulation using advanced driver models to generate human-
framework, a compact combination of behavioral and motion           like behavior with improved safety, and is validated through
planning. We first generate critical scenario sets conditioned      real-world autonomous driving experiments [15]. TPP [16]
on semantic-level ego policies using forward simulations. By        utilizes deep learning prediction models to generate MDP with
conducting the divergence assessment of future scenarios, we        better scalability. Nevertheless, drawbacks exist. For instance,
construct scenario trees with dynamic branchpoints. To diver-       the multipolicy-based pipelines only consider policies that
sify driving behaviors with tradeoffs between conservativeness      handle specific interactions in a single evolution and output
and efficiency, we introduce risk-aware contingency planning        the optimal policy over all scenarios, resulting in the lack of
(RCP) by advancing contingency planning with risk measure-          guarantee of policy consistency. Furthermore, existing meth-
ment. The proposed RCP generates trajectory trees consid-           ods only adopt single trajectory generation methods causing
ering diverse risk tolerances and future scenarios based on         unavoidable loss of multi-modality information.
efficient bi-level optimizations with linear programming (LP)
and the iterative linear quadratic regulator (iLQR). Finally,       B. Motion Planning with Contingency
the policy selection is conducted using the evaluation of both         Motion planning is another extensively studied area in
scenario trees and trajectory trees, thereby minimizing the loss    autonomous driving [17]. Techniques of motion planning
of multi-modal information. We demonstrate the promising            can be categorized into search-based methods [18]–[20],
ability of MARC through comparison with a strong baseline,          optimization-based methods [21]–[23], and a combinations of
quantitative ablation tests, and qualitative experiments on a       these two [24, 25]. Being an optimization-based method, con-
self-built multi-agent platform and the open-source CARLA           tingency planning generates deterministic actions for multiple
simulator [12]. The contributions of MARC include:                  future scenarios. In particular, it outputs trajectory trees with
   • MARC generates policy-conditioned critical scenarios           one shared segment and multiple branches to handle different
      and dynamically builds scenario trees that account for        problem setups independently. Using the model predictive
      the scene-level divergence for each policy.                   control (MPC) to solve the branching trajectory tree with one
   • Risk-aware contingency planning is introduced to effi-         shared state to track a desired path while considering the
      ciently generate diverse maneuvers which handle multi-        potential dangers, contingency MPC [26] shows the ability
      modal interactions under different risk tolerances.           to counteract potential threats in static environments. Con-
   • We conducted extensive quantitative experiments, com-          tingency planning can also generate deterministic actions,
      parisons with strong baselines, and qualitative experi-       accounting for the motion uncertainty of other agents when
      ments to showcase the capability of MARC.                     combined with probabilistic obstacle predictions [9]. Branch
   The remaining content is organized as follows. The related       MPC [10, 27] optimizes trajectory trees with multiple branch-
work is reviewed in Sec. II, and an overview of the proposed        points based on scenario trees, taking into account the potential
framework is presented in Sec. III. The methodologies are           future branching of other agents’ actions. Adapting similar
detailed in Sec. IV and Sec. V. Implementation details and          ideas, reactive iLQR [11] generates flexible yet safe maneuvers
experimental results are provided in Sec. VI and Sec. VII.          accounting for future environments without rigorously avoid-
Finally, the article is concluded in Sec. VIII.                     ing the whole reachable regions of other agents. However,
                                                                    existing methods have limitations in generating effective in-
                      II. R ELATED W ORK
                                                                    teractions in complex circumstances with low computational
A. Decision-making under uncertainty                                costs, primarily due to the lack of guidance from the decision-
   Extensive literature exists on addressing interactive planning   making layer. Additionally, these methods’ fixed or predefined
under uncertainty in dynamic scenarios. POMDP provides a            scenario trees do not generalize well to real-world applications.
theoretically sound framework for handling multiple potential          We address the above issues by systematically combining
futures in a general form. However, due to the curse of             multipolicy decision-making and contingency planning. Our
dimensionality, solving POMDPs in real-time is infeasible           framework generates a scenario tree that extracts the critical
as the driving context becomes more complex, even with              driving context for each semantic-level policy. Based on
advanced POMDP solvers [13, 14] and over-tailored prob-             the scenario tree, we achieve efficient risk-aware maneuver
lem modeling [2]–[4]. To further enable real-time decision-         generations accounting for critical scenarios with user-defined
making, multipolicy-based methods are proposed to simplify          risk tolerances leveraging contingency planning under risk
the POMDP process using domain-specific knowledge of                measurement. The compact design of our method enables
autonomous driving. Particularly, multipolicy-based methods         consistent policy generations under interaction uncertainty
prune the belief tree of the POMDP process using semantic-          while minimizing information loss between layers.
level policies which approximate the original action space and
retain the interaction among agents using closed-loop forward                          III. S YSTEM OVERVIEW
simulations. MPDM [5, 6], a representative of multipolicy-            As shown in Fig 2, we present a complete planning pipeline
based methods, shows a promising ability for interactive            that incorporates perception data from the simulator and
planning. Following the spirit of MPDM, EUDM [7] con-               generates optimal policy with trajectory trees for the vehicle
siders action and intention branching during the planning           controller. The vehicle controller evaluates certain target states
                                                                                                                                                              3



                                                                                             and interact according to pre-defined models. However, hand-
                                                                                             crafted models may diverge from reality, leading to unrealistic
                                                                                             simulations in extreme circumstances. We adopt the idea of
                                                                                             forward reachable sets(FRSs) [28, 29], which are used to
                                                                                             provide safety guarantees for ego trajectory planning and
                                                                                             extend them to capture the surrounding agents’ potential ten-
                                                                                             dencies in the simulation horizon. Therefore, the trajectory can
                                                                                             generate open-loop responses swerving away from potential
                                                                                             collision areas to ensure safety when closed-loop reactions
                                                                                             are infeasible. To further ensure safety, we introduce fallback
                                                                                             policies that take all FRSs as input and generate passive
                                                                                             maneuvers such as pull-over and emergent braking. Hence, we
                                                                                             ensure the completeness in the rendered scenario sets covering
                                                                                             the agents’ multi-modal behaviors conditioned on ego policies.
                                                                                                A typical scenario tree starts from the current state and
Fig. 2: Illustration of the MARC framework (marked in blue) and its relationship with        branches into different evolution at the same timestamp, rep-
other components. The ego vehicle is in cyan, and surrounding vehicles are in grey. In the
scenario set, the cyan arrows indicate the ego policies while the grey arrows indicate the
                                                                                             resenting the shared and independent solution spaces of the
intentions of surrounding vehicles. The solid lines represent the simulated trajectories.    rendered scenarios, respectively. Here, the process of building
In the scenario tree and trajectory tree, the branchpoint is in yellow, the shared part of
the trajectories is in green, and the contingency plans are in cyan.
                                                                                             the scenario tree is primarily concerned with determining the
                                                                                             latest timestamp at which the scenarios diverge. Delaying the
on the trajectory trees and computes the corresponding control                               branching time as much as possible offers two advantages.
signals. These control signals, along with those from other                                  Firstly, the number of state variables in the trajectory opti-
agents, are fed into the simulator, which synchronously prop-                                mization problem decreases as the duration of the shared part
agates the states according to the underlying physical models.                               increase. More importantly, a later branching point gives the
   MARC first predicts nearby vehicles’ future trajectories and                              ego vehicle more reaction time to handle different potential
semantic-level intentions based on historical observations and                               outcomes smoothly. Since the ego trajectories are the cumu-
environmental context. The critical scenario sets conditioned                                lative results of the interaction with the scene context, their
on the ego policies are then elected utilizing multi-agent for-                              differences can be used to reveal the divergence of scenarios.
ward simulation. The associated scenario trees with dynamic                                  Therefore, given the critical scenario set S, the corresponding
branch points are constructed utilizing scene-level divergence                               branch time calculation can be formally described as:
assessment. Taking the scenario trees, risk-aware contingency
planning (RCP) generates optimal maneuvers with the user-                                                     max τ ∈ {0, ..., τmax }
defined risk tolerances. Specifically, RCP solves a bi-level                                        s.t.   ∀M(si , sj , τ ) < θ,      (si , sj ) ∈ {S × S},
optimization consisting of LP for risk measurement and iLQR
for contingency planning. Leveraging the multi-modality in                                      where M is a function measuring the deviations between
both behavioral and motion planning layers, MARC generates                                   ego states at time τ given the scenario pairs (si , sj ) from the
actions that consider both risk and multiple future evolutions.                              Cartesian product of the scenario set. The state divergence
                                                                                             threshold is predefined as θ. To preserve the multi-modality
           IV. P OLICY- CONDITIONED S CENARIO T REE                                          and prevent the tree structure from collapsing into a single
   Using tree structures to represent scenarios provides a clear                             trajectory, a maximum branching time τmax is set to limit
topology of future evolution. As elaborated in Sec. II, fixed or                             the length of the shared part. Despite its simple design, the
predefined scenario trees have poor generalization. In contrast,                             proposed method is effective in practice. A brief depiction of
our approach involves constructing dynamic scenario trees that                               the divergence assessment can be found in Fig. 3a.
adapt to the change of interactions. The process is conducted
in a two-step manner: generating policy-conditioned critical                                         V. R ISK - AWARE C ONTINGENCY P LANNING
scenario sets via closed-loop forward simulations and building                               A. Prerequisites
tree representations with scene-level divergence assessments.
   We render scenarios based on the semantic-level ego poli-                                    Given the tree-structure descriptions of critical scenarios, we
cies and nearby agents’ intention predictions. The computa-                                  adopt contingency planning to cope with the multi-modality.
tional complexity of the process is bounded by only con-                                     As elaborated in Sec. II, contingency planning aims to obtain a
sidering policy-conditioned critical scenarios that might lead                               trajectory tree that accounts for all possible surroundings evo-
to potential risks. This pruning process is achieved by key                                  lutions. We define the planning problem with K contingency
vehicle selections with safety evaluations of non-cooperative                                plans as a constrained nonlinear optimization:
behaviors for each intention combination. Following previous
                                                                                                               X                   X X
                                                                                                          min      lj (xj , uj ) +        lj (xj , uj )
methods [6, 15], scenarios are rendered through closed-loop                                                U
                                                                                                                j∈Is                 k∈K j∈Ik
forward simulations using the selected vehicles with predicted
                                                                                                       s.t.     xi = f (pre(xi ), ui ),   ∀i ∈ I\{0}
intentions. Deterministic actions generated by the original
forward simulation assume all agents follow basic traffic rules                                                 hi (xi , ui ) ≤ 0,   ∀i ∈ I,
                                                                                                                                                             4



   where I, Is , and Ik are the index lists of all nodes,
shared nodes, and the nodes on the k-th contingency plan,
respectively. X := {xi |i ∈ I} is the trajectory states while
U := {ui |i ∈ I\{0}} is the control signals. The cost function,
constraint, and state-transition function are denoted by l(·),
h(·), and f (·), respectively. pre(·) returns the input node’s
predecessor. Fig. 3b depicts a general trajectory tree.

B. Bi-level Risk-aware Contingency Planning
   Due to the diverse risk tolerances, human drivers tend to         (a) scene divergence        (b) trajectory tree       (c) risk-neutral vs risk-averse

react differently, even given the same surrounding predictions.      Fig. 3: (a) depicts the branch time marked in yellow selected by divergence assessment.
                                                                     (b) shows a trajectory tree, where Is , and Ik are the index lists of shared nodes, and
Therefore, risk tolerances are essential for diverse maneu-          the nodes on the k-th contingency plan, respectively. (c) shows maneuvers generated
ver generations. We adopt conditional value-at-risk (CVaR),          by RCP with different risk tolerances α: risk-neutral (left) and risk-averse (right). The
                                                                     green trajectories are the shared trajectories. The branchpoints are colored in yellow.
which is recently introduced to contingency planning in [10],        Contingency plans and associated predictions of agents are marked in the same colors.
as a tail risk assessment measure in our framework. Given a
                                                                        iLQR utilizes the Taylor-series expansion to obtain approx-
discrete random variable of the distinct value set K with as-
                                                                     imated problems consisting of quadratic cost functions and
sociated probabilities {pk |k ∈ K} and risk costs {ξk |k ∈ K},
                                                                     discrete system transition functions of the original problem.
the CVaR can be defined as follows [30]:
                               X                                     Given fixed risk-related variables Q, we can obtain the ap-
              CVaRα = max          qk pk ξk                          proximated subproblem for control sequence U as follows:
                              Q
                                                                           X 1                            1
                  s.t.   0 ≤ qk ≤ (1 − α)−1 , k ∈ K                   min       ( xTj Rj xj + rjT xj + uTj Sj uj + sTj uj )+
                         X                                             U          2                       2
                             qk pk = 1,                                    j∈Is
                                                                      X X               1                        T
where Q := {qk |k ∈ K} is a series of variables that                             pk qk ( xTj Rjsaf e xj + rjsaf e xj )+
                                                                                        2
                                                                      k∈K j∈Ik
weigh the original risk cost under constraints. α ∈ [0, 1) is         X X 1                                   T       1
a hyperparameter that decides the percentage of worse cases                     ( xTj Rj−saf e xj + rj−saf e xj + uTj Sj uj + sTj uj )
that the risk measure should take into consideration.                             2                                   2
                                                                      k∈K j∈Ik
   To combine CVaR with contingency planning, we treat the                  s.t.   xi = Ai pre(xi ) + Bi ui ,              ∀i ∈ I\{0},
safety part of cost functions lsaf e as the risk costs, which
                                                                                       ∂f                   ∂f
will be weighted by the CVaR variables Q and leave the               where Ai ≡ ∂pre(x     i)
                                                                                              and Bi ≡ ∂u     i
                                                                                                                . This approximated
other cost functions l−saf e unbiased. The resulting risk-aware      problem could be solved by iLQR in the discrete dynamic
contingency planning (RCP) is shown as follow:                       programming style using backward and forward propagation.
             X                                                       We refer the interested readers to [31].
   max min       lj (xj , uj )+                                        Note that for any fixed control sequence U , the original
    Q     U
              j∈Is
              X X                                                  problem degenerates into an LP. Solving the LP finds the
                  pk qk ljsaf e (xj , uj ) + lj−saf e (xj , uj )     CVaR with the optimal variables Q under an extra equality
              k∈K j∈Ik                                               constraint. For any fixed Q, the original problem can be
        s.t. xi = f (pre(xi ), ui ),    ∀i ∈ I\{0}                   approximated by iLQR. Solving the iLQR subproblem obtains
                                                                     optimal controls U of the trajectory tree. Since both subprob-
              hi (xi , ui ) ≤ 0,   ∀i ∈ I
                                                                     lems are convex, we can derive a bi-level optimization with a
              0 ≤ qk ≤ α−1 , k ∈ K                                   convergence guarantee:
              X
                  qk pk = 1,
                                                                               U k+1 = argmin iLQR X k , U k , Qk
                                                                                                                      
                                                                                           U
   By changing α, which indicates the risk tolerance levels of
                                                                               Qk+1 = argmax LP X k+1 , U k+1 , Qk .
                                                                                                                        
users, we can obtain diverse driving behaviors through RCP.                                         Q
As shown in Fig. 3c, if setting α → 0, we can obtain risk-
neutral maneuvers with the optimal variables Q closed to given          Owing to the low cost of solving LP and iLQR, the compu-
probabilities. Conversely, choosing α → 1 brings risk-averse         tational efficiency of the risk-aware contingency planning can
maneuvers with increased optimal variables Q weighting on            be guaranteed. Compared with original contingency planning,
the dangerous contingency plans.                                     RCP generates trajectories that are optimal in multiple future
                                                                     scenarios under user-defined risk-averse levels, which can give
   The above problem can be solved in its dual form by intro-        rise to more human-like behaviors.
ducing dual variables [10]. However, this method increases the
problem dimension and scales poorly to the high-dimensional
state-transition function. To further improve the computational      C. Policy Evaluation
efficiency, we adopt the bi-level optimization method in which          As stated in Sec. II, existing multipolicy-based pipelines
the solution to the original problem is obtained by iteratively      usually require a subsequent motion planner that only consid-
solving two subproblems using iLQR and LP.                           ers a single selected scenario, leading to potential oscillations
                                                                                                                                                5




  Algorithm 1: Process of MARC                                              probabilities. We note that it is possible to predict both future
                                                                            trajectories and intentions using a single network, which we
   Inputs : Perception Z, Ego vehicle state x, Ego
                                                                            leave as future work.
             policy set Π, Horizon τh , Max Branch Time
             τmax , Divergence threshold θ
                                                                            B. Ego Policy Design
   Outputs: Optimal policy π ∗ , Optimal trajectories T ∗
 1 R ← ∅ ; //init policy reward set                                           Considering achieving diverse driving behaviors, we define
 2 T ← ∅ ; //init policy trajectory set                                     semantic-level policies for the ego vehicle, which can be cate-
 3 P, I ← PredictIntentionTrajectories(Z, x) ;                              gorized as longitudinal and lateral policies. Both longitudinal
 4 Ic ← GetIntentionCombination(I) ;                                        and lateral policies encompass basic semantic-level policies
 5 for π ∈ Π do                                                             and scene-related policies. For example, in urban driving,
       /* Scenario Generation */                                            the basic longitudinal policies could be maintaining speed,
 6     Sπ ← ∅ ; //init scenario set                                         decelerating, and accelerating. Scene-related policies could
 7     for c ∈ C do                                                         be stopping at a specific position (i.e., stop line and conflict
 8         Vk ← GetKeyVehicles(Z, c, x, π) ;                                zone), decelerating to yield, and accelerating to the speed limit.
 9         Sπ ← ForwardSimulation(Z, c, Vk , x, π, τh ) ;                   Similarly, the lateral policies can be defined as basic policies
10     end                                                                  such as lane keeping, lane change to the target lane sequence,
11     Ψπ ← ScenarioTreeConstruction(Sπ , τmax , θ) ;                       and scene-related policies such as bypass and in-lane siding.
12     Tπ ← RcpOptimization(π, Ψπ , τh ) ;
13     T ← Tπ ;                                                             C. Forward Simulation with FRSs
14     R ← PolicyEvaluation(π, Ψπ , Tπ ) ;                                     Existing work usually obtains reachable sets with model-
15 end                                                                      based propagation or over-approximations [34, 35]. In this
     ∗   ∗
16 π , T ← PolicySelection(R, T ) ;                                         work, we adopt a simple yet effective method to generate
                                                                            FRSs. Specifically, we directly compute the spatial-temporal
in the final output when dealing with rapidly changing sce-                 occupancy of multi-modal predictions and collect the occu-
narios. In contrast, our framework conducts policy selection                pancy union according to the timestamp to form the FRSs.
based on the evaluation of both scenario trees and trajectory               Since FRS is essentially a spatial-temporal occupation de-
trees, ensuring the consistency of the resulting policies.                  scription without probabilistic features and the trajectory pre-
   The flow of the MARC framework is detailed in Algo 1.                    dictions have implicitly encoded the estimation of the hidden
Policy-conditioned scenarios are rendered under each policy                 state as well as the transition model of the surrounding agents,
based on the intention predictions (Line 7 to Line 10). The                 our method can approximate these occupations with little
scenario trees and trajectory trees are obtained consecutively              computational cost while preserving multi-modality.
from the critical scenario sets (Line 11 to Line 12). Evaluations              The forward simulation takes two steps. We first conduct
are conducted based on both scenario trees and trajectory trees             the deterministic closed-loop forward simulation similar to [7]
(Line 14). Most parts of our framework (Line 5 to Line 15)                  with safety assessment nts on the generated trajectories. The
can be run in parallel for real-time performance.                           failure of the assessments will trigger the second simulation
                                                                            where we replace those failed policies with our fallback
                 VI. I MPLEMENTATION D ETAILS                               policies. By running the second forward simulation, we can
                                                                            obtain the final trajectories with closed-loop reactions to
A. Trajectory and Intention Prediction
                                                                            nearby agents and open-loop reactions to FRSs.
   We adopt a neural network-based motion predictor based
on [32] to provide scene-centric multi-modal trajectory pre-                D. iLQR Design
diction for all nearby agents. The network is trained on the
                                                                               We adopt the kinematic bicycle model as the state-transition
Argoverse dataset [33] and deployed in C++ via LibTorch1 .
                                                                            function f (·) in iLQR and design the cost function of i-th node
After obtaining the predicted trajectories and their probability
                                                                            li as the sum of safety cost lisaf e , target cost litar , kinematic
scores, the next step is to transform them into semantic-level
                                                                            cost likin and comfort cost licomf :
intentions. In this work, the longitudinal intentions of other
agents are defined as a set of semantic actions (i.e., maintain-                            li = lisaf e + litar + likin + licomf .
ing speed, acceleration, and deceleration), while the lateral
                                                                               Similar to [23], environmental constraints can be modeled
intentions are represented as the reference lane sequences. To
                                                                            utilizing vectorized representations (such as polygons for
extract the lateral intentions, a simple voting method is used
                                                                            drivable areas and polylines for reference lines). We introduce
based on the predicted trajectories’ positional distribution. In
                                                                            a distance function D(·) that returns the Euclidean distance
particular, we begin by extracting candidate reference lanes for
                                                                            between the ego vehicle and the environmental constraint. To
the target agent and then associate each predicted trajectory
                                                                            ensure the second-order continuity, we further define a smooth
with these candidate lanes to evaluate the likelihood of each
                                                                            function with parameter β as follows:
lane. Once the lateral intention is obtained, we use a similar
                                                                                           Ω
method to estimate the longitudinal intentions and associated                              X    exp(βγi )γi
                                                                                Sβ (Γ) =       PΩ               , Γ = {γ1 , γ2 , ..., γΩ }T .
  1 The C++ distribution of PyTorch. See https://pytorch.org for details.                  i=1  i=1 exp(βγ  i )
                                                                                                                                                              6



   With a slight abuse of notation, we denote G(·) as the user-
designed penalty function. The safety cost penalizes the signed
distances to multiple environmental constraints:
                                 drivable area
                      z          }|           {
                                              
            lisaf e = Gdri 1 − 1Pd Sβ<0 D(Pd ) +
                                  
                                                                           (a) cut-in            (b) noise injection               (c) dynamic traffic

            bounding box                         reachable set      Fig. 4: Illustration of the scenario setups. (a) shows the snapshots of the cut-in scenario.
 zX             }|             { z         }|             {         The ego vehicle is expected to yield to the non-cooperative vehicle. (b) illustrates the
                                X                                 noise injection (marked in yellow) on the predictions (c) is a typical dynamic traffic
        Gbb 1Pbj Sβ<0 (D(Pbj )) +  Grs 1Prj Sβ<0 (D(Prj )) ,        scenario in which the ego vehicle has to interact with other agents under uncertainty.
 j∈np                               j∈np

where Pd defines a drivable area that is feasible and collision-
free to static obstacles, Pbj is the i-th inflated bounding boxes
of j-th surrounding agent, Prj is the FRS of j-th surrounding
agent and 1(·) denotes the enclosed indicator function. Similar
formulations can be found in the domain of backup policy
control barrier function quadratic programming [36].
   The target term varies depending on the policies and the
scenarios. For instance, the reference can be a centerline
that serves as a guide for lane change policies in highway
scenarios, while alternatively representing a desired area for
overtaking at an intersection. For simplicity, we divide this
term into two parts: reference cost and desired area cost:
   litar = Gref Sβ<0 D(Lref ) + Gdes Sβ<0 D(Pdes ) ,
                                                          
                                                                    Fig. 5: Dynamic profiles of the comparisons with EPSILON. In Scen.I, MARC captures
                                                                    the cut-in intention and generates smoother deceleration during the cut-in period marked
                                                                    in yellow. In Scen.II, the additional noise is added to the predictions from 2.5s to 6s
where Lref is the reference polyline of target lane sequence,       marked in yellow. MARC handles the disturbances while EPSILON switches the policy
Pdes is the target area defined by polygon.                         which results in uncomfortable deceleration. In Scen.III, MARC outperforms EPSILON
                                                                    with better efficiency and riding comfort.
   The kinematic constraints are introduced by penalizing the
state and control variables that exceed the limits:
                                                                                         VII. E XPERIMENTAL R ESULTS
 likin = Gxub max(xt − xub , 0) + Gxlb max(xlb − xt , 0)
                                                         
                                                                  A. Simulation Platform and Environment
       + Guub max(ut − uub , 0) + Gulb max(ulb − ut , 0) ,
                                                                      The experiments are conducted on a self-built multi-agent
where xub , xlb are the upper and lower bound of the state          simulation platform elaborated in Sec. III and CARLA. The
variables, while uub , ulb are the upper and lower bound of the     proposed MARC framework is implemented in C++11 with
control inputs, max(·) is the element-wise max function.            customized solvers. All experiments are run on a desktop
  The final term of the objective function is designed to           computer equipped with Intel i9-12900K CPU and 32GB
minimize both the longitudinal discomfort (i.e., acceleration       RAM with a stable running rate of 20Hz.
and jerk) and the lateral discomfort (i.e., lateral acceleration
and steering rate), thereby ensuring the smoothness of the          B. Quantitative Results
generated trajectories:
                                                                       We conduct comparisons with the existing strong baseline
                   licomf = Glon (xt ) + Glat (xt ).                and ablation tests on our self-built platform to evaluate the
                                                                    performance of the proposed method quantitatively.
                                                                       1) Comparison with EPSILON: We conduct qualitative
E. Policy Evaluation
                                                                    comparisons with the existing strong baseline EPSILON in
  In this work, the total policy reward is evaluated based          the three scenarios. Scen.I is a non-cooperative cut-in scenario
on the scenario contexts and risk-aware contingency plans.          aiming to test the ability to handle extreme cases. Scen.II
Specifically, the reward of policy is defined as the negative       is a noise injection test where the adjacent vehicle remains
weighted sum of several components:                                 lane-keeping during the test while noise is injected into the
        R = −(λ1 Fs + λ2 Fe + λ3 Fn + λ4 Fr + λ5 Fu ),              prediction module of the ego vehicle, increasing the lane-
                                                                    changing prediction, as shown in Fig. 4b. This setup simulates
where the safety cost Fs is measured by the distance between        the case where predictions deviate from the ground truth,
trajectories of ego and other vehicles. The efficiency cost Fe      which is common in real-world applications. Scen.III is a
is computed from the gap between the average velocity of            highway scenario with multiple autonomous agents, which
contingency plans and desired velocity. The navigation cost Fn      is set up to assess the planning ability under intensive un-
is obtained by the difference between the routing and target        certain interactions. As shown in Fig. 4c, the ego vehicle is
lane sequence. The risk cost Fr is obtained from the cost of        assigned a global route and a speed limit to navigate through
RCP, and the uncertainty cost Fu is measured by the branch          the traffic in which agents will conduct various behaviors.
time and the divergence between different contingency plans.        For fair comparisons, we use the same prediction module
                                                                                                                                                                                      7




Fig. 6: Key frames of the qualitative experiments. The global route is marked by yellow arrows, key vehicles are marked by red circles, and the simulated trajectories are marked
by grey bands with darkness representing probabilities. (a-d) Unprotected left turn: ego vehicle first yields to the aggressive left-turning vehicle (a), slowly moves forward to stop
the oncoming vehicle on the left meanwhile yields to the second vehicle on the right (b-c), and finally pass around the stopped vehicle to exit the intersection (d). (e-h) Bypass
with oncoming traffic: ego vehicle first swerves to the left, trying to bypass the parked vehicle (e), once anticipating the oncoming vehicle is not likely to yield, ego vehicle aborts
bypassing and turn into the original lane for collision avoidance (f), and it quickly recovers the bypass policy as soon as the spatiotemporal constraints are met (g-h).

                   TABLE I: Comparison with EPSILON                                                                 TABLE II: Ablation Study Results

                               Time      Avg Spd       RMS Acc         Max Abs Acc                                            Avg Max Dec           Avg Min Dis          Suc Rate
         Methods                                                                                Methods
                                (s)       (m/s)         (m/s2 )          (m/s2 )                                                (m/s2 )                 (m)                (%)
               EPSILON         19.90        6.87           0.63              1.03               w/o branch                          3.48                  1.37                83
  Scen. I
               MARC            12.56        7.26           0.45              0.75
                                                                                                Fixed branch                        2.52                  2.53                93
               EPSILON         8.70         7.90           0.72              1.13
  Scen. II
               MARC            7.60         8.03           0.51              0.78               Dyna branch                         1.63                  3.45                96
               EPSILON         54.46        7.41           0.70              2.78               Dyna branch + Risk                  1.14                  4.12               100
  Scen. III
               MARC            53.13        7.61           0.34              1.09
                                                                                              one owing to the better adaptation to the scenarios. Combining
for both methods. To show that the policies of MARC are                                       risk measures with dynamic branching brings a more defensive
more robust under uncertain interactions and bring fewer                                      yet smoother driving style which achieves the best perfor-
unexpected policy switches, which results in better efficiency                                mance among methods, proving the effectiveness of RCP.
and comfort, we use total time and average speed during
the test for efficiency measurement and use the root mean                                     C. Qualitative Results
squared acceleration and maximum absolute acceleration for                                       We conduct qualitative tests in highly interactive scenarios
comfort assessment [37, 38]. As shown in Table I, our method                                  such as unprotected left turns and bypass with oncoming traffic
outperforms the baseline with improvements in all metrics.                                    on the self-built platform and scenarios with random traffic on
The dynamic profiles are shown in Fig 5 to show side-by-side                                  CARLA to further verify the generalization of our method.
comparisons in three scenarios. MARC, being more cautious                                        1) Unprotected left turn: The ego vehicle enters a 3-way
when facing multiple predictions, accelerates smoother and                                    intersection with no traffic signals. Vehicles from different
decelerates in time in Scen.I. In Scen.II, MARC executes more                                 directions are randomly generated, and their actual intentions
consistent policies under noisy predictions, whereas EPSILON                                  are unknown to the ego vehicle. As shown in Fig 6 (a-d), the
switches its policies resulting in sudden decelerations. In                                   ego vehicle first slows down to yield to the first left-turning
Scen.III, MARC handles the uncertain interactions better than                                 vehicle (a), creeps forward to stop the vehicle on the left while
EPSILON, with fewer unnecessary decelerations and a higher                                    giving the right of way to the second vehicle on the right (b-
average speed, improving comfort and efficiency.                                              c), and finally exits the intersection with a swerving maneuver
   2) Ablative tests of RCP: We modify the cut-in scenario 4a                                 to avoid the potential collision (d). The result shows MARC’s
for the ablation test, in which the ego vehicle only executes the                             ability to handle uncertain intentions and generate reasonable
lane-keeping policy, and the neighboring vehicle conducts a                                   behaviors during negotiations.
non-cooperative lane-change maneuver forcing the ego vehicle                                     2) Bypass with oncoming traffic: This scenario consists of a
to yield. A baseline planner, a fixed branching mechanism,                                    parked vehicle and a fast-moving vehicle in the opposite lane.
a dynamic branching mechanism, and a dynamic branching                                        As shown in Fig 6 (e-h), the ego vehicle first swerves to the
mechanism with risk-aware optimization methods will be                                        left, attempting to conduct a left bypassing maneuver on the
tested in this scenario 100 times with random initialization. We                              parked vehicle (e), once noticing that the oncoming agent is
use scene-specific metrics for comfort and safety assessments:                                less likely to yield, it aborts bypassing policy and returns the
average maximum deceleration, average minimum distance                                        original lane for collision avoidance (f), and after holding to
with respect to the cut-in vehicle, and success rate(non-                                     yield for a while, it switches back to the bypass policy once it
collision cases vs. total cases). Metrics such as time to collision                           anticipates the spatial-temporal constraints are satisfied (g-h).
and intervehicular time are also applicable for safety assess-                                The result reveals MARC’s ability to efficient behavior gener-
ments [39]. As shown in Table II, the baseline only reacts after                              ations under spatial-temporal constraints in conflict situations.
the most likely prediction aligns with the cut-in behaviors,                                     3) Qualitative tests on CARLA: We extend our method to
resulting in the hardest deceleration, closest distance, and the                              CARLA and conduct tests on random traffic flows. As shown
lowest success rate. Branching mechanisms enables planners                                    in Fig 7, MARC generates human-like driving behaviors such
to response earlier, smoothing the deceleration and increasing                                as defensive left-turning and bypassing under negotiation in
the minimum distance. Dynamic branching outperforms fixed                                     the tests, proving its generalization in various scenarios.
                                                                                                                                                                             8



                                                                                              [15] W. Ding, L. Zhang, J. Chen, and S. Shen, “EPSILON: An efficient plan-
                                                                                                   ning system for automated vehicles in highly interactive environments,”
                                                                                                   IEEE Trans. on Robot., vol. 38, no. 2, pp. 1118–1138, 2021.
                                                                                              [16] Y. Chen, P. Karkus, B. Ivanovic, X. Weng, and M. Pavone, “Tree-
                                                                                                   structured policy planning with learned behavior models,” ICRA. IEEE,
                                                                                                   pp. 7902–7908, 2023.
                                                                                              [17] D. González, J. Pérez, V. Milanés, and F. Nashashibi, “A review of
                                                                                                   motion planning techniques for automated vehicles,” IEEE Trans. on
                                                                                                   Intel. Trans. Syst., vol. 17, no. 4, pp. 1135–1145, 2015.
Fig. 7: Keyframes of tests on CARLA. The historical positions and trajectories of             [18] M. Rufli and R. Siegwart, “On the design of deformable input-/state-
surrounding agents are marked with transparent colors. (a-b)Ego vehicle achieves human-            lattice graphs,” in ICRA. IEEE, 2010, pp. 3071–3077.
like defensive left-turning behaviors in intersection with random traffic. (c-e)Ego vehicle   [19] M. McNaughton, C. Urmson, J. M. Dolan, and J.-W. Lee, “Motion plan-
achieves bypassing with negotiation behaviors.                                                     ning for autonomous driving with a conformal spatiotemporal lattice,”
                                                                                                   in ICRA. IEEE, 2011, pp. 4889–4895.
                                                                                              [20] L. Ma, J. Xue, K. Kawabata, J. Zhu, C. Ma et al., “Efficient sampling-
              VIII. C ONCLUSION AND F UTURE W ORK                                                  based motion planning for on-road autonomous driving,” IEEE Trans.
                                                                                                   on Intel. Trans. Syst., vol. 16, no. 4, pp. 1961–1976, 2015.
   We present the MARC framework as a solution for au-                                        [21] W. Xu, J. Wei, J. M. Dolan, H. Zhao, and H. Zha, “A real-time motion
tonomous driving in dynamic interactive environments. Our                                          planner with trajectory optimization for autonomous vehicles,” in ICRA.
framework systematically unified two innovative techniques,                                        IEEE, 2012, pp. 2061–2067.
                                                                                              [22] J. Ziegler, P. Bender, T. Dang, and C. Stiller, “Trajectory planning for
policy-conditioned scenario tree and risk-aware contingency                                        Bertha—a local, continuous method,” in IV. IEEE, 2014, pp. 450–457.
planning, to generate safe and interactive driving behaviors                                  [23] J. Chen, W. Zhan, and M. Tomizuka, “Constrained iterative lqr for on-
considering risk tolerance and uncertainty. Comparisons with a                                     road autonomous driving motion planning,” in ITSC. IEEE, 2017, pp.
                                                                                                   1–7.
strong baseline and comprehensive experiments in challenging                                  [24] T. Gu, J. Atwood, C. Dong, J. M. Dolan, and J.-W. Lee, “Tunable and
scenarios proved the effectiveness of our design. In the future,                                   stable real-time trajectory planning for urban autonomous driving,” in
we will conduct real-world field tests for MARC.                                                   IROS. IEEE, 2015, pp. 250–256.
                                                                                              [25] H. Fan, F. Zhu, C. Liu, L. Zhang, L. Zhuang, D. Li, W. Zhu, J. Hu,
                                                                                                   H. Li, and Q. Kong, “Baidu Apollo EM motion planner,” arXiv preprint
                                   R EFERENCES                                                     arXiv:1807.08048, 2018.
                                                                                              [26] J. P. Alsterda, M. Brown, and J. C. Gerdes, “Contingency model
 [1] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and                            predictive control for automated vehicles,” in ACC. IEEE, 2019, pp.
     acting in partially observable stochastic domains,” Art. Intel., vol. 101,                    717–722.
     no. 1-2, pp. 99–134, 1998.                                                               [27] R. Wang, M. Schuurmans, and P. Patrinos, “Interaction-aware model
 [2] W. Liu, S.-W. Kim, S. Pendleton, and M. H. Ang, “Situation-aware                              predictive control for autonomous driving,” in ECC. IEEE, 2023, pp.
     decision making for autonomous driving on urban road using online                             1–6.
     POMDP,” in IV. IEEE, 2015, pp. 1126–1133.                                                [28] S. Kousik, S. Vaskov, F. Bu, M. Johnson-Roberson, and R. Vasudevan,
 [3] C. Hubmann, J. Schulz, M. Becker, D. Althoff, and C. Stiller, “Auto-                          “Bridging the gap between safety and real-time performance in receding-
     mated driving in uncertain environments: Planning with interaction and                        horizon trajectory design for mobile robots,” Intl. J. Robot. Res., vol. 39,
     uncertain maneuver prediction,” IEEE Trans. on Intel. Veh., vol. 3, no. 1,                    08 2020.
     pp. 5–17, 2018.                                                                          [29] S. Vaskov, U. Sharma, S. Kousik, M. Johnson-Roberson, and R. Vasude-
 [4] Y. Luo, P. Cai, A. Bera, D. Hsu, W. S. Lee, and D. Manocha,                                   van, “Guaranteed safe reachability-based trajectory design for a high-
     “PORCA: Modeling and planning for autonomous driving among many                               fidelity model of an autonomous passenger vehicle,” in ACC. IEEE,
     pedestrians,” IEEE Robot. Autom. Lett., vol. 3, no. 4, pp. 3418–3425,                         2019, pp. 705–710.
     2018.                                                                                    [30] R. Rockafellar, S. Uryasev, and M. Zabarankin, “Deviation measures in
 [5] A. G. Cunningham, E. Galceran, R. M. Eustice, and E. Olson, “MPDM:                            risk analysis and optimization,” SSRN Electronic Journal, 12 2002.
     Multipolicy decision-making in dynamic, uncertain environments for                       [31] D. Mayne, “A second-order gradient method for determining optimal
     autonomous driving,” in ICRA. IEEE, 2015, pp. 1670–1677.                                      trajectories of non-linear discrete-time systems,” Intl. J. Ctrl., vol. 3,
 [6] E. Galceran, A. G. Cunningham, R. M. Eustice, and E. Olson, “Multi-                           no. 1, pp. 85–95, 1966.
     policy decision-making for autonomous driving via changepoint-based                      [32] T. Zhao, Y. Xu, M. Monfort, W. Choi, C. Baker, Y. Zhao, Y. Wang,
     behavior prediction: Theory and experiment,” Auton. Robot., vol. 41,                          and Y. N. Wu, “Multi-agent tensor fusion for contextual trajectory
     no. 6, pp. 1367–1382, 2017.                                                                   prediction,” in CVPR, 2019, pp. 12 126–12 134.
 [7] L. Zhang, W. Ding, J. Chen, and S. Shen, “Efficient uncertainty-aware                    [33] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
     decision-making for automated driving using guided branching,” in                             D. Wang, P. Carr, S. Lucey, D. Ramanan et al., “Argoverse: 3D tracking
     ICRA. IEEE, 2020, pp. 3291–3297.                                                              and forecasting with rich maps,” in CVPR, 2019, pp. 8748–8757.
 [8] P. O. Scokaert and D. Q. Mayne, “Min-max feedback model predictive                       [34] S. Kousik, S. Vaskov, M. Johnson-Roberson, and R. Vasudevan, “Safe
     control for constrained linear systems,” IEEE Trans. Autom. Ctrl.,                            trajectory synthesis for autonomous driving in unforeseen environ-
     vol. 43, no. 8, pp. 1136–1142, 1998.                                                          ments,” in DSCC. ASME, vol. 58271, 2017.
 [9] J. Hardy and M. Campbell, “Contingency planning over probabilistic                       [35] I. Hwang, D. M. Stipanovic, and C. J. Tomlin, “Applications of polytopic
     obstacle predictions for autonomous road vehicles,” IEEE Trans. on                            approximations of reachable sets to linear dynamic games and a class
     Robot., vol. 29, no. 4, pp. 913–929, 2013.                                                    of nonlinear systems,” in ACC. IEEE, vol. 6, 2003, pp. 4613–4619.
[10] Y. Chen, U. Rosolia, W. Ubellacker, N. Csomay-Shanklin, and A. D.                        [36] Y. Chen, M. Jankovic, M. Santillo, and A. D. Ames, “Backup control
     Ames, “Interactive multi-modal motion planning with branch model                              barrier functions: Formulation and comparative study,” in CDC. IEEE,
     predictive control,” IEEE Robot. Autom. Lett., vol. 7, no. 2, pp. 5365–                       2021, pp. 6835–6841.
     5372, 2022.                                                                              [37] C. Pek and M. Althoff, “Fail-safe motion planning for online verification
[11] F. Da, “Comprehensive reactive safety: No need for a trajectory if you                        of autonomous vehicles using convex optimization,” IEEE Trans. on
     have a strategy,” in IROS. IEEE, 2022, pp. 2903–2910.                                         Robot., vol. PP, 12 2020.
[12] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,                           [38] M. Elbanhawi, M. Simic, and R. Jazar, “In the passenger seat: Investi-
     “CARLA: An open urban driving simulator,” in Proceedings of the 1st                           gating ride comfort measures in autonomous cars,” ITSM. IEEE, vol. 7,
     Annual Conference on Robot Learning, 2017, pp. 1–16.                                          no. 3, pp. 4–17, 2015.
[13] N. Ye, A. Somani, D. Hsu, and W. S. Lee, “DESPOT: Online POMDP                           [39] S. Glaser, B. Vanholme, S. Mammar, D. Gruyer, and L. Nouveliere,
     planning with regularization,” J. Artif. Intel. Res., vol. 58, pp. 231–266,                   “Maneuver-based trajectory planning for highly autonomous vehicles
     2017.                                                                                         on real road with traffic and driver interaction,” IEEE Trans. on Intel.
[14] H. Kurniawati and V. Yadav, “An online POMDP solver for uncertainty                           Trans. Syst., vol. 11, no. 3, pp. 589–606, 2010.
     planning in dynamic environment,” in ISRR. Springer, 2016, pp. 611–
     629.
