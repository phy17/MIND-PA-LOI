                                                                                                                                                                  2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)
                                                                                                                                                                  October 19-25, 2025. Hangzhou, China




                                                                                                                                                                   Multimodal Integrated Prediction and Decision-making with Adaptive
                                                                                                                                                                                    Interaction Modality Explorations
2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) | 979-8-3315-4393-8/25/$31.00 ©2025 IEEE | DOI: 10.1109/IROS60139.2025.11247130




                                                                                                                                                                                                          Tong Li1∗ , Lu Zhang1∗ , Sikang Liu2 , Shaojie Shen1


                                                                                                                                                                     Abstract— Navigating dense and dynamic environments poses
                                                                                                                                                                  a significant challenge for autonomous driving systems, owing
                                                                                                                                                                  to the intricate nature of multimodal interaction, wherein the
                                                                                                                                                                  actions of various traffic participants and the autonomous
                                                                                                                                                                  vehicle are complex and implicitly coupled. In this paper, we
                                                                                                                                                                  propose a novel framework, Multimodal Integrated predictioN
                                                                                                                                                                  and Decision-making (MIND), which addresses the challenges
                                                                                                                                                                  by efficiently generating joint predictions and decisions cov-
                                                                                                                                                                  ering multiple distinctive interaction modalities. Specifically,
                                                                                                                                                                  MIND leverages learning-based scenario predictions to obtain
                                                                                                                                                                  integrated predictions and decisions with socially-consistent
                                                                                                                                                                  interaction modality and utilizes a modality-aware dynamic
                                                                                                                                                                  branching mechanism to generate scenario trees that efficiently
                                                                                                                                                                  capture the evolutions of distinctive interaction modalities                   Fig. 1: A branch of the generated scenario tree and its corresponding topological
                                                                                                                                                                  with low growth of interaction uncertainty along the planning                  structure. In MIND, we employ a learning-based scene-consistent driver model coupled
                                                                                                                                                                                                                                                 with the adaptive interaction modality exploration (AIME) mechanism to efficiently
                                                                                                                                                                  horizon. The scenario trees are seamlessly utilized by the                     construct the scenario tree. For each branch originating from the root node, we utilize
                                                                                                                                                                  contingency planning under interaction uncertainty to obtain                   contingency planning to generate a trajectory tree, accommodating multimodal future
                                                                                                                                                                  clear and considerate maneuvers accounting for multimodal                      evolutions.
                                                                                                                                                                  evolutions. Comprehensive experimental results in the closed-
                                                                                                                                                                  loop simulation based on the real-world driving dataset show-                  plan on other agents. However, these hierarchical approaches
                                                                                                                                                                  case superior performance to other strong baselines under                      fail to model the implicit bidirectional interaction and would
                                                                                                                                                                  various driving contexts. Code is available at: https://
                                                                                                                                                                                                                                                 potentially lead to unrealistic predictions and decisions, such
                                                                                                                                                                  github.com/HKUST-Aerial-Robotics/MIND.
                                                                                                                                                                                                                                                 as over-conservative or over-optimistic behaviors. Recently,
                                                                                                                                                                                           I. I NTRODUCTION                                      joint multi-agent motion forecasting models [10]–[13] have
                                                                                                                                                                                                                                                 been widely studied, which focus on predicting multiple
                                                                                                                                                                     While autonomous driving technology has made remark-
                                                                                                                                                                                                                                                 possible future scenarios that are physically and socially
                                                                                                                                                                  able strides recently, navigating through dense and dynamic
                                                                                                                                                                                                                                                 consistent given the driving context. These approaches em-
                                                                                                                                                                  traffic remains a formidable challenge. Generating safe and
                                                                                                                                                                                                                                                 ploy deep neural networks to implicitly capture the inherent
                                                                                                                                                                  smooth maneuvers in such situations requires accurate mod-
                                                                                                                                                                                                                                                 dependencies and interactions among agents. Typically, when
                                                                                                                                                                  eling of interaction among agents and reasoning about how
                                                                                                                                                                                                                                                 the ego vehicle is integrated into the model, the network is
                                                                                                                                                                  the scenario evolves in the future, which is non-trivial
                                                                                                                                                                                                                                                 also capable of predicting its future trajectories. Although
                                                                                                                                                                  since the intentions of agents are inherently multimodal and
                                                                                                                                                                                                                                                 these predictions could inform the ego vehicle’s decisions
                                                                                                                                                                  often coupled with each other, even with perfect perception
                                                                                                                                                                                                                                                 to some extent, directly using them for planning results in
                                                                                                                                                                  results [1]–[3].
                                                                                                                                                                                                                                                 suboptimal performance. One reason is that, despite mod-
                                                                                                                                                                     Extensive research has been conducted to address the
                                                                                                                                                                                                                                                 ern networks’ ability to model interactions between agents
                                                                                                                                                                  challenge by introducing learning-based integrated predic-
                                                                                                                                                                                                                                                 and static scenes effectively, long-term and scene-consistent
                                                                                                                                                                  tion and planning systems. Some existing approaches adopt
                                                                                                                                                                                                                                                 prediction remains difficult [10, 13]. The uncertainty of
                                                                                                                                                                  explicit hierarchical modeling and address these two tasks
                                                                                                                                                                                                                                                 predictions always escalates after just a few seconds due
                                                                                                                                                                  separately, with one serving as the conditional input for
                                                                                                                                                                                                                                                 to the inherent multimodality, which leads to unreliable
                                                                                                                                                                  the other. For instance, following the “predict-then-plan”
                                                                                                                                                                                                                                                 decisions. On the other hand, plain joint motion forecasting
                                                                                                                                                                  pipeline, [4]–[6] generate multimodal motion prediction for
                                                                                                                                                                                                                                                 models struggle to produce desired trajectories for the ego
                                                                                                                                                                  all agents in the scene, then leverage them as input for
                                                                                                                                                                                                                                                 vehicle without extra guidance, highlighting a limitation in
                                                                                                                                                                  the following trajectory planning task. In contrast, [7]–[9]
                                                                                                                                                                                                                                                 their application [14].
                                                                                                                                                                  sample goals or trajectories of the ego vehicle in advance
                                                                                                                                                                                                                                                    For sequential decision-making, tree search has been
                                                                                                                                                                  and then use them as additional conditions for the motion
                                                                                                                                                                                                                                                 widely applied, which considers the dynamics of the world
                                                                                                                                                                  prediction network, aiming to model the influence of ego
                                                                                                                                                                                                                                                 and investigates its evolution into the future. Traditional
                                                                                                                                                                    ∗ Contributed equally to this work.                                          decision-making approaches frame problems as partially ob-
                                                                                                                                                                    1 T. Li, L. Zhang and S. Shen are with the Department of Electronic and
                                                                                                                                                                                                                                                 servable Markov decision processes (POMDPs) and employ
                                                                                                                                                                  Computer Engineering, Hong Kong University of Science and Technology,          tree search techniques to derive sub-optimal solutions [3, 15].
                                                                                                                                                                  Hong Kong (e-mail: tlibm@ust.hk; lzhangbz@ust.hk; eeshaojie@ust.hk).
                                                                                                                                                                    2 S. Liu is with the ZYT Technology Company, Ltd., Shenzhen 518057,          Given the challenges in scaling and the sometimes intractable
                                                                                                                                                                  China (e-mail: sikang.liu@zyt.com).                                            nature of modeling behaviors and interactions of driving

                                                                                                                                                                  979-8-3315-4393-8/25/$31.00 ©2025 IEEE                                   13674
                                                                                                                                                                          Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
agents in complex environments through human heuristics                       achieving high prediction accuracy for a single agent is
and handcrafted rules, contemporary approaches incorporate                    insufficient, while ensuring physical and social consistency
neural networks to model transition and observation func-                     among all participants is equally paramount. For multi-
tions [14, 16, 17]. This adaptation facilitates the generation                agent joint prediction, [4, 22] initially generate marginal
of human-like interactive driving maneuvers in various situ-                  predictions for each agent, which are then integrated using
ations. However, existing methods often rely on a fixed tree                  a deep structured model to deduce the joint distribution of
structure or explicitly decouple the prediction and planning                  behaviors. Factorization-based approaches [23, 24] tackle the
processes, resulting in limited flexibility.                                  joint prediction by explicitly establishing a partial order of
   To address the above limitations, we introduce the Multi-                  target agents, and then modeling the problem as a conditional
modal Integrated predictioN and Decision-making (MIND), a                     prediction task. Implicit methods [10]–[13], by directly fore-
novel method that systematically combines a learning-based                    casting the joint possible future with minimal assumptions
integrated prediction and planning model, a dynamic branch-                   and inductive biases, offer enhanced generalizability and
ing mechanism, and contingency planning on multimodal                         improved computational efficiency. We follow the implicit
future evolutions, enabling the generation of reasonable                      methods and adopt an efficient joint motion predictor based
behaviors in complex interaction scenarios. In MIND, we                       on our previous work [25].
adopt a lightweight and efficient joint multi-agent motion
prediction network designed to produce scene-consistent fu-                   B. Integrated Prediction and Planning
ture distributions for both the ego vehicle and all surrounding                  Partially observable Markov decision process (POMDP)
agents. In the planning phase, this network acts as the “world                offers a mathematically rigorous approach to modeling un-
dynamics”, enabling the construction of scenario trees by its                 certainties and multi-agent interactions, but its high com-
recursive invocation. To ensure comprehensive coverage of                     putational complexity makes it challenging for existing
pivotal scenarios while avoiding redundant search efforts, we                 solvers [3, 15] to meet the real-time requirements of
introduce a dynamic branching mechanism named Adaptive                        decision-making tasks in autonomous driving. Some ap-
Interaction Modality Exploration (AIME), which utilizes the                   proaches achieve satisfactory results in real systems by sim-
uncertainty in the predicted states of agents to guide the                    plifying the original problem using domain knowledge [26,
branching process. To identify the optimal decision, we                       27]. However, these systems grounded in human experience
evaluate each branch originating from the root node and                       often exhibit limited flexibility in complex environments and
find the most advantageous one. In line with our previous                     pose challenges for scalability. Alongside, there’s a pivot
work [18], we determine the policy by evaluating both sce-                    towards learning-based integrated systems for overcoming
nario and trajectory trees produced by contingency planning.                  prediction and planning challenges. These systems vary from
This approach allows for adherence to various constraints                     hierarchical models, where prediction sequentially informs
and cost functions, thereby improving the ability to handle                   planning [4]–[6], to approaches that integrate the ego ve-
scene uncertainty. A typical illustration can be found in                     hicle’s intended actions into motion prediction, considering
Fig. 1. We validate the effectiveness of MIND by conducting                   their effects on other agents [7, 8]. Despite advancements,
comprehensive experiments via open-loop and closed-loop                       such models often miss capturing bidirectional agent in-
simulations. The results demonstrate its superior performance                 teractions, occasionally resulting in impractical behaviors.
in diverse driving scenes compared to other baselines, un-                    Additionally, recent methods [14, 16, 17] incorporating
derscoring its potential to facilitate autonomous driving in                  neural networks for modeling the “world dynamics” show
complex environments. We summarize the contributions of                       promise for creating realistic, interactive maneuvers. In this
this paper as follows:                                                        paper, we follow and enhance this pipeline by introducing an
   • We design a scene prediction network and integrate                       adaptive branching mechanism, leading to higher flexibility
     it with tree search techniques featuring a dynamic                       and efficiency.
     branching mechanism, resulting in a scenario tree with
     enhanced coverage for exploring the world’s evolutions.                  C. Motion Planning with Contingency
   • For the multiple potential futures within the scenario                      Contingency planning is introduced to produce determin-
     tree, we utilize contingency planning to naturally gener-                istic actions that consider the motion uncertainty of other
     ate optimal trajectory trees against each branch originat-               agents [28]. To address potential changes in the intentions of
     ing from the root, thereby determining the best decision.                others, [29, 30] implement a scenario tree with a predefined
   • We evaluate MIND through various experiments, with                       topological structure. This is succeeded by optimization over
     results that outperform other baselines across diverse                   the scenario tree using model predictive control to derive
     scenarios, showing its efficacy in complex situations.                   a trajectory tree that captures reactive behaviors in future
                                                                              steps. Combined with multipolicy decision-making [26],
                     II. R ELATED W ORK
                                                                              [18] introduces risk-aware contingency planning on policy-
A. Joint Multi-agent Motion Prediction                                        conditioned scenario trees with dynamic branching points for
   Previous studies mostly focus on predicting a single target                each policy. In this paper, we extend this idea by optimizing a
agent given its surrounding context [19]–[21]. However, con-                  multi-way scenario tree dynamically constructed by a neural
sidering that the behaviors of road users are interdependent,                 network to ascertain the optimal decision for the ego vehicle.

                                                                        13675
       Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
                                                                                      spatial predictions through linear Gaussian dynamics. We
                                                                                      simplify the description here for clarity. For further details
                                                                                      on this conversion process, we kindly refer interested readers
                                                                                      to [9]. Under the linear dynamic assumption, the positional
                                                                                      distribution of any agent at a given future time step can
                                                                                      be obtained by propagating from the previous positional
 Fig. 2: Illustration of the components and the workflow in the MIND framework.
                                                                                      Gaussian and the predicted action Gaussian, facilitating the
                   III. F RAMEWORK OVERVIEW                                           recursive branching for the expansions of the scenario tree.
   Fig 2 depicts the proposed MIND framework, which                                      To improve the long-term prediction accuracy, the network
consists of two key procedures: dynamically building the                              can also incorporate high-level planning commands which
scenario tree with AIME and policy evaluation. Utilizing                              are defined as the intended route in MIND. These commands,
the observations and environmental data, MIND efficiently                             which can be aligned with ground truth during training and
creates a scenario tree through AIME-guided branching. The                            generated on-board, are introduced into the scenario decoder
tree explores the future interaction modalities, incorporating                        rather than the fusion network to prevent bias in agent inter-
both predictions and decision-making processes simultane-                             actions. The network design and implementations are further
ously. The selection of the optimal policy is determined by                           detailed in Sec. VI. Note that while the high-level commands
assessing the scenario evolutions within each branch that                             serve as options for enhanced conditioned predictions, the
stems from the root node, along with the trajectory tree                              long-term generations of desired ego decisions are primarily
obtained from contingency planning for handling multimodal                            guided by the subsequent pruning and merging procedure.
future interactions. Details are provided in Sec. IV to Sec. VI.                      The effectiveness of the network in both unconditioned and
                                                                                      conditioned scenario prediction tasks is evaluated in Sec. VII.
IV. A DAPTIVE I NTERACTION M ODALITY E XPLORATIONS
A. Integrated Prediction and Decision
                                                                                      B. Branching Decision based on Uncertainty Variation
   As elaborated in Sec. II, learning-based methods can gen-
erate joint predictions of agents and the ego vehicle, namely                            Accurately anticipating the scenarios with single-shot pre-
scenario predictions. In MIND, we leverage a transformer-                             dictions is difficult due to the agent intentions’ multimodality
based network to anticipate the scenario in the form of                               and coupling over time and situations [2, 10]. With the
Gaussian mixture models (GMMs). We first denote the map                               “world dynamics”, the implicit transition and observation
information as M, the historical observations as X, which                             functions of both agents and the ego vehicle learnt by the
contains the observed trajectories of Na moving agents and                            network, it is intuitive to explore multiple possible evolutions
ego vehicle over the past H time steps. The network gen-                              at different time steps, namely branching, to obtain dis-
erates joint distributions Y consisting of agents’ predictions                        tinctive joint distributions under different interactions. How-
and ego decisions in the prediction horizon T :                                       ever, brute-force branching with a fixed time interval leads
                                                                                      to computational inefficiency and exponential complexity.
    P (Y | X, M) = P (Y | Z, X, M) P (Z | X, M) ,                           (1)
                                                                                      Recognizing that interaction changes affect agents’ future
where Z are latent variables that capture unobserved features                         actions, manifesting as increased covariance in GMMs, we
(e.g., agent intentions, driving styles, and interactions). To                        conduct an evaluation on Yk to dynamically determine a
ensure clarity, we define the scenario node Ytk of the k-th                           branching time step tkb :
predicted scenario at time step t as follows:
                                                                                                 tkb = min{t ∈ Z+ | U(Ytk ) ≥ β},                  (6)
  P Ytk | X, M = αk N µk,i           k,i
                 
                               t , Σ t     , i ∈ {e, 1, . . . , Na },
                                                                   (2)                where U is the measuring function which evaluates the
                                                                                      change rate of variation and β is a customized tolerance
with αk ,    µk,i
              t   and Σk,i
                       t   representing the probability score,                        of uncertainty to achieve a trade-off between evolution
mean and covariance of the positional Gaussian N of a cer-                            diversity and computational efficiency. If the determined
tain agent or ego vehicle, respectively. The joint distribution                       branching point tb falls within the planning horizon T ,
of agents’ positions at time step t can be expressed as a                             a branching process is executed. This branching process
GMM:                                                                                  updates a pseudo-observation X̄ with the means of current
                             PK
                                                                                      predicted GMM components and      generates the resulting
                                                   
          P (Yt | X, M) = k=1 P Ytk | X, M ,                (3)
                                                                                      possible scenarios P Y | X̄, M leveraging the prediction
The k-th predicted scenario Yk and the overall scenario                               network and the attributes of GMMs under linear dynamics.
predictions Y are defined as below:                                                   If no branching point is found within T , the predictions are
                    Yk = {Ytk }, t ∈ {1, ..., T },                          (4)       truncated to T and marked as the end scenarios. This strategy
                                k                                                     imitates the reasoning process of human drivers: when the
                    Y = {Y }, k ∈ {1, ..., K},                              (5)
                                                                                      deduced future interactions are too uncertain, human drivers
To be more specific, the network estimates the Gaussian over                          would reflect on existing modalities and pick a suitable time
actions of the single integrator, which are then converted into                       step to deduce more certain modes into the future.

                                                                                  13676
        Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
                                                                                              Algorithm 1: Branching with AIME
                                                                                               Inputs : M, X, T , β, δ, dmax
                                                                                               Outputs: Scenario Tree Ψ
                                                                                            1 N0 ← {X, 0}, E ← ∅, B ← {N0 } ; //Init Node
                                                                                            2 while B ̸= ∅ do
                                                                                             3    B̄ ← ∅ ;
                                                                                             4    for N ∈ B do
                                                                                                      /* AIME Iteration */
                                                                                             5        X, d ← N ;
                                                                                             6        if d ≤ dmax then
                                                                                             7            Y ← ScenarioPrediction(X, M) ;
Fig. 3: Illustration of one AIME-guided branching. The nodes of the scenario tree            8            Ȳ ← Pruning&Merging(Y, δ) ;
contain the states of the ego vehicle and agents. Firstly, the scenario tree is extended     9            for Yk ∈ Ȳ do
on the branching node leveraging the scenario prediction network. Then, the extended
scenario tree is simplified by the pruning and merging process according to the                               /* Branching Decision */
interaction modality analysis. Finally, end nodes and branching nodes are determined       10                 tkb ← GetBranchTime(Yk , β) ;
during the adaptive branching, which triggers the next AIME process if branching
nodes exist.                                                                               11                 if tkb < T then
                                                                                            12                      X̄ ← UpdateObser(X, Yk ) ;
C. Pruning and Merging based on Interaction Modality                                       13                 else
   The network’s multimodal nature might inadvertently                                      14                      X̄ ← TruncatePred(X, Yk ) ;
lead to the generation of undesired maneuvers, potentially                                 15                 end
compromising long-term decisions. Additionally, preserving                                                    /* Create New Node */
all similar expanded scenarios without differentiation leads                               16                 N̄ ← {X̄, d + 1} & AddNode(N̄ , N ) ;
to exponentially increasing computational complexity when                                  17                 if tkb < T then
extracting critical actions from scenario trees. Therefore, the                             18                      B̄ ← B̄ + {N̄ } ;
predicted scenario should be further evaluated and processed,                              19                 else
guiding the development of the scenario tree towards the                                    20                      E ← E + {N̄ } ;
desired evolutions. We introduce a pruning process to discard                              21                 end
scenarios with deviated ego decisions and low probabilities.                               22             end
On the other hand, we observe that the agents in similar                                   23         end
scenarios exhibit similar interactions and end at adjacent                                 24     end
final positions, which can be well categorized by the recently                             25     B ← B̄ ;
proposed free-end homotopy [29]. For an ego-agent pair, the                                26 end
homotopy class he→i is defined as follows:                                                 27 Ψ ← GetScenarioTree(E) ;
                                            j e→i       k
  ∆de→i =                               = ∆dδ + 12 , (7)
             P e→i             
                 dt − de→i t−1 ∼ , h
                                    e→i
                                                                                           computational efficiency by limiting the search depth and
                                                                                           pruning rare modalities. In practice, we assign a maximum
where de→i
        t     is the angle between the mean positions of an                                branch depth dmax to AIME and abandon the rare interaction
ego-agent pair at time step t, (·)∼ normalizes the angle                                   modalities, which have high uncertainties and require exces-
difference to (−π, π], δ is the quantization factor of homo-                               sive branching to resolve. An AIME iteration is illustrated
topy class, ⌊·⌋ is the floor function that rounds the resulting                            in Fig 3 and the complete methodology is detailed in Algo 1.
value down to the nearest integer. We define the interaction
modality I of scenario Y based on the homotopy classes of                                       V. E VALUATION VIA C ONTINGENCY P LANNING
ego-agent pairs:                                                                              As the scenario tree unfolds across various interaction
                 I(Y) := he→1 , ..., he→Na .
                                             
                                                            (8)                            modalities over time, policies that handle diverse future
                                                                                           evolutions naturally arise. Specifically, we define the GMM
Given the scenarios of the same interaction modality, the                                  ego decision sequence, spanning from the root node to the
merging process picks the one with the highest probability                                 end nodes in the sub-tree, as a policy generated by the AIME-
as the representative and discards the others while summing                                guided scenario tree, illustrated by the expanded sub-tree
up their probabilities. The probabilities of scenarios are                                 in Fig 1. For a given policy, it is necessary to determine de-
normalized across the scenario tree to ensure consistency.                                 terministic actions that effectively address multimodal agent
   The iterative process of branching, pruning, and merging                                predictions for further evaluation and execution. Following
forms a scenario tree that adaptively explores the interaction                             our previous work [18], we utilize contingency planning,
modality space, a procedure we term Adaptive Interaction                                   a compact yet efficient solution for handling multiple evo-
Modality Explorations (AIME). The resulting AIME-guided                                    lutions. We extend this technique to incorporate integrated
trees feature more distinctive agent behaviors with lower                                  decisions and predictions in scenario trees in the form of
uncertainty in each step. This benefits subsequent contin-                                 GMMs. We denote the number of predicted scenarios in the
gency planning while preserving a compact structure for                                    sub-tree by Ns . Given the j-th scenario, we denote the index

                                                                                      13677
          Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
of its preceding scenario by j̄, its branch time by tjb , the full
time step set of it by Tj = {tj̄b +1, ..., tjb } and a finite set that
excludes the first time step Tj− = Tj \{tj̄b + 1}. For j = 1,
we have tj̄b = 0, which refers to the time step of the root
node. With a slight abuse of notation, we define the state
and control of the trajectory tree in the j-th scenario at time
step t by xjt and ujt , the set of states by X and the set of                  Fig. 4: Architecture of the scene prediction network. After the feature encoding, we
control actions by U . The trajectory tree τ is obtained as                    mix the scene-level mode queries with agent features. As illustrated, A = 3 agents
                                                                               are included in this scene, while K = 6 mode queries are injected. At last, the scene
follows:                                                                       decoder generates K possible joint future scenes with estimated probabilities.
                PNs P             j  j    j         j   j
      τ := min j=1        t∈Tj (lt (xt , ut ) + γnt (xt ))         (9)    different interaction modalities or “consensus” among traffic
            U
       1
 s.t. x1 = f (xˆ0 , u11 ),                            ∀j ∈ {2, ..., Ns }, participants. We mix the mode queries with all agent features
                             xjj̄ = f (xj̄j̄ , ujj̄+1 ),
                              tb +1                                       and send them to an MLP-based scene decoder to generate
      xjt = f (xjt−1 , ujt ), t ∈ Tj− , ∀j ∈ {1, ..., Ns },               K possible future scenes. Specifically, in a driving scenario
        j  j    j                                                         with A agents, we replicate the fused agent features K
      ht (xt , ut ) ≤ ⃗0, t ∈ Tj , ∀j ∈ {1, ..., Ns },
                                                                        times and the mode queries A times, respectively. After that,
      P gtj (xjt , ujt ) ≤ 0 ≥ 1 − p, t ∈ Tj , ∀j ∈ {1, ..., Ns },        both tensors have the shape of [K, A, D], where D is the
                                                                          latent size, and we can then aggregate them to get the agent
where njt is the negative log-likelihood (NLL) loss with features under each scene. Moreover, we parameterize the
respect to the Gaussian distribution of the ego deci- predicted trajectories using GMMs in the decoder, reflecting
sion weighted by a non-negative factor γ. f (·) is the the motion uncertainty. As mentioned in Sec. IV, the high-
state-transition function, hjt (·) is the deterministic multi- level commands, if given, are encoded and directly injected
dimensional constraint function, ltj (·) is the customized loss, into the scene decoder to obtain the conditioned prediction
the first three constraints ensure the trajectory tree starts of ego decisions. This approach avoids involving high-level
from the state of the root node xˆ0 and the continuity of the commands in global feature fusion, namely, the commands
trajectory tree within the scenarios and between scenarios of the ego vehicle only affect its own fused features, thereby
and their predecessor, and gtj (·) is the safety constraint preventing incorrect message passing and unnecessary de-
function defined on the Gaussian distributions of agent pendencies.
predictions. P returns the probability of the input function,                We train this network in an end-to-end manner, leveraging
and p is a tolerance of constraint violation probability. The a combined regression and classification loss. We utilize
problem above can be integrated with risk measurement such the scene-level winner-takes-all strategy [12] to avoid mode
as conditional value-at-risk to form a risk-aware contingency collapse. As for the classification loss, we employ the max-
planning problem [18], aiming to develop actions averse to margin loss [19] to distinguish the winner scene from others.
potential dangers.
   Solving this extended problem given different sub-trees B. iLQR Design
obtains the trajectory trees accounting for multimodal evolu-                We solve the contingency problem utilizing the iterative
tions under associated policies. The optimal policy and tra- linear quadratic regulator (iLQR) [31]. We adopt the discrete
jectory tree are then chosen based on the reward evaluation: bicycle kinematic model as f (·) and design the loss function
                                                                  (10) of iLQR, as shown below:
                            PNs P
                Q(τ ) :=        j=1        R(xj , uj ),
                                        t∈Tj        t      t
               ∗
             τ = argmax Q(τi ), i ∈ {1, ..., Nτ },                   (11)                   lt = ltsafe + lttar + ltkin + ltcomf + ltdec + ltcol ,            (12)
                         τ

in which Q(·) computes the total reward over all states in the                 in which ltsafe , lttar , likin , and licomf are the safety cost, target
trajectory tree. The reward function is detailed in Sec. VI.                   cost, kinematic cost, and comfort cost respectively. These
                                                                               cost components align with those defined in MARC [18]. To
                VI. I MPLEMENTATION D ETAILS                                   tailor iLQR for the extended contingency planning problem,
A. Scene-level Prediction Network                                              we incorporate two additional cost elements: ltdec , which
   As shown in Fig 4, the prediction network follows the                       calculates the NLL loss on the ego decisions’ GMMs, and
encoder-decoder architecture that takes the map information                    ltcol , which penalizes the potential collision based on GMM
and historical observations as inputs and generates multiple                   predictions. We define ltdec using the Mahalanobis distance
future scenes and their probability scores. For the context                    measure function D(·) on the Gaussian distribution, as the
encoding part, we adopt our previous work [25], which                          square of Mahalanobis distance is proportional to the NLL:
first encodes the observed trajectories of surrounding road                                                   ltdec = D2 (Nte ).                              (13)
users and map elements, and then performs efficient global
feature fusion using a Transformer-like network. To achieve                    For enforcing safety constraints, limiting collision probabili-
consistent scene prediction, we introduce K scene-level                        ties with other agents is effectively managed by setting mini-
mode queries in the decoding procedure, which represent                        mum thresholds for the Mahalanobis distance. Consequently,

                                                                         13678
        Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
we specify the potential collision penalty ltcol as:                          in NN+CP and MIND are aligned for equitable comparison.
                PNa                                                           The closed-loop experiments are conducted on a self-built
         ltcol = j=1  G max(Dbnd − D(Ntj ), 0) ,
                                                    
                                                                    (14)
                                                                              multi-agent platform based on the Argoverse 2 dataset. This
where G(·) represents the custom penalty function and Dbnd                    platform operates synchronously, in which the perception
denotes the threshold for the Mahalanobis distance such that                  is rendered according to the vehicles’ positions and the
P{D ≤ Dbnd } = 1 − p for the distribution Ntj from the                        observations from the data, and the states of the simulated
GMMs of the j-th agent’s prediction at time step t.                           vehicles are updated according to the kinematic model and
                                                                              planned trajectories in each step. Our experiments, including
C. Reward Function                                                            baselines, our proposed system, and the simulation platform,
   To select trajectory trees that effectively balance efficiency,            are implemented in Python3. Closed-loop simulations are run
comfort, and commonness, we propose a multi-dimensional                       on a desktop with an Intel i5-12500KF CPU and an Nvidia
reward function to evaluate the states, controls, and proba-                  RTX 3060 GPU. Network training is conducted with a batch
bilities associated with the trajectory tree τ outlined below:                size 128 for 50 epochs on a server with 8 Nvidia RTX 3090
                                                                              GPUs.
           R(xjt , ujt ) = λp (λ1 Fs + λ2 Fe + λ3 Fc ).             (15)
where λp likelihood-related weight for customized prefer-                     B. Results
ence on commonness, λ1 , λ2 , and λ3 denote non-negative                         1) Quantitative comparison with state-of-the-arts on
weights, Fs assesses the safety by evaluating the Maha-                       multi-agent prediction task: We compare the proposed net-
lanobis distance to other agents’ predictions, Fe evaluates                   work with other state-of-the-art methods based on scene-
efficiency by comparing the planned velocity against the                      level interaction modeling to validate its performance. Given
target velocity, and Fc quantifies comfort based on the                       that our network generates GMM-based outputs that differ
planned control.                                                              from the trajectory outputs typical of multi-agent forecasting
                                                                              tasks, we recalibrate by mapping the GMM predictions to
               VII. E XPERIMENTAL R ESULTS
                                                                              trajectories using the predicted means and probabilities. The
A. Experiment Setup                                                           quantitative results of the multi-agent motion forecasting
   1) Dataset and simulations: Our experiments are con-                       benchmark on Argoverse 2 are shown in Table I. Despite
ducted on the Argoverse 2 [32] motion forecasting dataset,                    not primarily targeting trajectory predictions, our method
which offers 10-Hz sequences including 5 seconds of his-                      outperforms baseline methods, showcasing its efficacy in
torical data and 6 seconds of future motion predictions, and                  multi-agent interaction modeling.
are accompanied by high-definition maps. We conduct multi-
agent trajectory prediction evaluation, effectiveness analysis,               TABLE I: Results on the test split of the Argoverse 2 multi-world
                                                                              forecasting benchmark. The best result is in bold.
and closed-loop simulations based on the Argoverse dataset.
   2) Metrics: For prediction evaluations, we utilize stan-                        Methods           MinADE6        MinFDE6        actorMR6       actorCR6
dard metrics for multi-agent trajectory predictions: average                       FJMP [24]            0.81           1.89           0.23           0.01
minimum average displacement error (MinADE), average                               FFINet [13]          0.77           1.77           0.24           0.02
minimum final displacement error (MinFDE), actor miss                              Proposed             0.70           1.62           0.20          0.009
rate (actorMR), and actor collision rate (actorCR) [32]. The
MinADE measures the mean lowest L2 norm to the ground
truth, while MinFDE focuses on the endpoint error. The
actorMR measures the average deviation ratio of predic-
tions for each scored agent across the evaluation set. The
actorCR is the ratio of collisions among agents within the
scenario of the lowest MinFDE. For effectiveness analysis
of AIME, we evaluate the modality coverage, number of
predicted scenarios in the scenario tree, and relative com-                   Fig. 5: Three challenging intersection scenarios from Argoverse 2 used for closed-
                                                                              loop evaluation (ego vehicle in blue). Each scenario features complex and ambiguous
putational cost. For closed-loop simulation evaluations, we                   interactions: (I) a three-way encounter with misleading intentions; (II) a two-way
utilize typical planning metrics: average speed, maximum                      encounter requiring yielding intention recognition; and (III) an unprotected left turn
                                                                              with an ambiguous oncoming vehicle.
absolute acceleration, and root-mean-squared acceleration.
The avgSpd measures the overall efficiency, maxAbsAcc                            2) Effectiveness analysis of AIME: We conduct quantita-
captures the uncomfortable maneuvers, and rmsAcc reflects                     tive experiments to evaluate the effectiveness of AIME in
the decision consistency.                                                     three types of scenarios: highway, street and intersection.
   3) Baseline, Platform and Environment: For quantitative                    We randomly selected 50 data sequences of each type from
closed-loop comparisons, we benchmark against two models:                     the validation split of Argoverse 2. To obtain a complete
a model-based prediction and decision-making module with                      set covering diverse interaction modalities for comparisons,
contingency planning (MD+CP) similar to MARC [18] and                         a brute-force search (BF-SRCH) is adopted where pre-
a learning-based variant in which single-shot results are used                dicted scenarios are expanded in a fixed-time-step manner.
for prediction and decision (NN+CP). Both neural networks                     Meanwhile, a single shot (SS) method in which interaction

                                                                        13679
       Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
Fig. 6: Snapshots of the effectiveness analysis in the intersection scenario. Predictions and decisions in the same scenarios are colored in the same color. The associated scenario
trees are visualized on the right. (a) Single Shot: With no branching on the scenario tree, the predicted uncertainties of the oncoming agent increase sharply upon entering the
intersection. The predictions covering a large area of intersections place great challenges to the following contingency planning. (b) AIME: The prediction uncertainties are kept
relatively low and the interaction patterns are clear within the sparse scenario tree. Compared with the Single Shot and Brute-force Search, AIME achieves good coverage on
interaction modalities and better efficiency on scenario tree generations. (c) Brute-force Search: By branching with a fixed time gap, an exhaustive search is conducted to find
future scenarios covering possible interaction modalities. The resulting scenario tree is complicated and computationally expensive to integrate with contingency planning.


         TABLE II: Results of Effectiveness Study of AIME.                                  TABLE III: Quantitative results of closed-loop simulations in three
                                                                                            test driving scenarios. The better result is in bold.
              Methods                  Cover. (%)       Scen.Num.         Comp.Cost
                                                                                                                                 avgSpd          maxAbsAcc             rmsAcc
                     SS                93.6             6                 1.0x                         Methods
   Highway           AIME              96.4             11.4              2.7x
                                                                                                                                (m/s) ↑           (m/s2 ) ↓           (m/s2 ) ↓
                     BF-SRCH           100.0            7776              1974.8x                               MB+CP              4.00               1.53                0.75
                     SS                79.2             6                 1.0x                 Scen. I          NN+CP              3.42               1.65                0.84
   Street            AIME              88.9             34.3              4.6x                                  MIND               4.25               0.89                0.59
                     BF-SRCH           100.0            7776              2567.7x                               MB+CP              3.83               1.01                0.82
                     SS                17.8             6                 1.0x                 Scen. II         NN+CP              3.25               1.30                0.98
   Intersection      AIME              79.5             197.1             17.2x                                 MIND               4.14               0.99                0.76
                     BF-SRCH           100.0            7776              3179.5x                               MB+CP              2.24               1.22                0.67
                                                                                               Scen. III        NN+CP              2.42               1.43                0.74
                                                                                                                MIND               2.63               1.04                0.66
modalities are obtained with one inference is provided as
a baseline for comparison. For fair comparisons, neural
networks and the hyperparameters of interaction modality
are consistent in the three methods. The averaged quantitative
results are in Table II. AIME shows notable efficiency and
modality coverage in the scenarios tested, outperforming the
SS method which fails to cover enough interaction modalities
due to the significant prediction uncertainties, and the BF-
SRCH method, which suffers from inefficiency due to the
                                                                                            Fig. 7: A snapshot of the closed-loop simulation in Scen.I. The historical trajectories are
expansion of superfluous scenarios. The predicted scenarios                                 colored in the fading purple. The 2-sigma ellipses of the GMMs, predicted trajectories,
of three methods in an intersection are shown in Fig 6.                                     and planned trajectories are visualized in light blue. MIND effectively identifies the
                                                                                            interaction pattern where the oncoming vehicle may gradually advance to allow passing
   3) Quantitative comparisons in closed-loop simulations:                                  and executes a side-passing maneuver to overtake. Conversely, MB+CP predicts a less
                                                                                            realistic scenario where both vehicles simultaneously attempt to give way, attributed
We further conduct closed-loop quantitative comparisons                                     to the handcrafted models’ limited ability, leading to conservative slowing down.
with the baselines across three typical scenarios, as shown
in Fig 5, selected from the aforementioned intersection                                     worst-case analysis, we heighten the risk and urgency of the
group. In these cases where the intentions and right-of-                                    scenarios by assigning aggressive policies (such as sudden
ways need to be determined promptly and precisely for safe                                  accelerations, abrupt changes in direction from straight to
and smooth navigation, the comparisons can showcase the                                     turning or forcing right-of-way changes) to the adversarial
superiority of MIND. As shown in Table II, MIND performs                                    agents. The results, as illustrated in Fig 8, reveal MIND’s
better in all three scenarios. Compared with MB+CP, MIND                                    capacity to adapt to the evolving intentions of other agents
anticipates futures with better scene-consistent predictions                                and make considerate decisions with human-like behaviors,
and decisions, leading to more reasonable actions Fig 7.                                    demonstrating its adaptability.
Meanwhile, thanks to the multimodal interactions explored                                      5) Qualitative results of conditioned scenario predictions:
with the guidance of AIME given the “world dynamics”, the                                   To illustrate the network’s proficiency in generating dis-
uncertainties in each scenario are effectively narrowed down,                               tinct predictions tailored to different planning objectives,
enabling less conservative and more interaction-appropriate                                 we conduct a qualitative analysis with different high-level
maneuvers of MIND.                                                                          commands as conditioned inputs. The input routes vary
   4) Qualitative results of closed-loop simulations: We con-                               based on the specified high-level commands throughout these
duct qualitative experiments to evaluate MIND’s capability                                  tests, yet the historical data and map information remain
of interaction handling in multi-agent scenarios with diverse                               unchanged. As shown in Fig 9, the proposed network suc-
behaviors. To achieve this, we modify the scenarios from                                    cessfully produces a variety of plausible scenarios tailored
the quantitative analysis by incorporating adversarial agents                               to each command, highlighting its capability for conditioned
that exhibit dynamic actions. Additionally, taking the idea of                              prediction and flexibility in responding to various commands.

                                                                                      13680
            Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
Fig. 8: Snapshots of closed-loop simulations with adversarial agents (green). (a-c) In Scen. I, facing two adversaries—one turning left aggressively and another accelerating
to take right-of-way—MIND correctly anticipates both actions, yields sequentially, and then safely proceeds. (d-f) In Scen. III, when an agent unexpectedly goes straight with
aggressive acceleration instead of turning, MIND adapts by creeping forward cautiously before completing its turn after the agent has passed.

                                                                                            [10] S. Casas et al., “Implicit latent variable model for scene-consistent
                                                                                                 motion forecasting,” in ECCV. Springer. Springer, 2020, pp. 624–
                                                                                                 641.
                                                                                            [11] R. Girgis et al., “Latent variable sequential set transformers for
                                                                                                 joint multi-agent motion prediction,” arXiv preprint arXiv:2104.00563,
                                                                                                 2021.
                                                                                            [12] J. Ngiam et al., “Scene Transformer: A unified multi-task model for
                                                                                                 behavior prediction and planning,” arXiv preprint arXiv:2106.08417,
                                                                                                 vol. 2, no. 7, 2021.
Fig. 9: Qualitative results of scenario predictions conditioned on different high-level     [13] M. Kang et al., “FFINet: Future feedback interaction network for
commands. Each command generates a distinct interactive scenario: For “Turning left”,            motion forecasting,” arXiv preprint arXiv:2311.04512, 2023.
the ego vehicle decelerates as the oncoming car proceeds without yielding. For “Going       [14] Y. Chen et al., “Tree-structured policy planning with learned behavior
straight”, the oncoming car waits for the ego to pass. For “Turning right”, the ego turns        models,” in ICRA. IEEE. IEEE, 2023, pp. 7902–7908.
and then yields to a pedestrian.                                                            [15] Y. Luo et al., “PORCA: Modeling and planning for autonomous
                                                                                                 driving among many pedestrians,” IEEE Robot. Autom. Lett., vol. 3,
            VIII. C ONCLUSION AND F UTURE W ORK                                                  no. 4, pp. 3418–3425, 2018.
                                                                                            [16] Z. Huang et al., “DTPP: Differentiable joint conditional prediction and
   We introduce the MIND framework as a comprehensive                                            cost evaluation for tree policy planning in autonomous driving,” arXiv
approach for simultaneous prediction and decision-making                                         preprint arXiv:2310.05885, 2023.
                                                                                            [17] R. Chekroun et al., “MBAPPE: MCTS-built-around prediction for
in autonomous driving. The framework systematically com-                                         planning explicitly,” arXiv preprint arXiv:2309.08452, 2023.
bines a scenario prediction network, adaptive interaction                                   [18] T. Li et al., “MARC: Multipolicy and risk-aware contingency planning
modality exploration mechanism, and contingency planning                                         for autonomous driving,” IEEE Robot. Autom. Lett., 2023.
                                                                                            [19] M. Liang et al., “Learning lane graph representations for motion
to generate reasonable behaviors in complex interaction                                          forecasting,” in ECCV. Springer. Springer, 2020, pp. 541–556.
scenarios while handling multimodal future evolutions. Ex-                                  [20] J. Gao et al., “VectorNet: Encoding hd maps and agent dynamics from
tensive quantitative comparisons against state-of-the-art and                                    vectorized representation,” in CVPR, 2020, pp. 11 525–11 533.
                                                                                            [21] B. Varadarajan et al., “MultiPath++: Efficient information fusion and
qualitative experiments have shown the superiority of our                                        trajectory aggregation for behavior prediction,” in ICRA. IEEE, 2022,
approach. However, the framework involves hyperparameters                                        pp. 7814–7821.
that require tuning, and its reliance on vectorized represen-                               [22] W. Luo et al., “Jfp: Joint future prediction with interactive multi-agent
                                                                                                 modeling for autonomous driving,” in CoRL, 2023, pp. 1457–1467.
tations for scene elements limits direct integration into end-                              [23] Q. Sun et al., “M2I: From factored marginal trajectory prediction to
to-end systems. Future work will focus on evolving it into a                                     interactive prediction,” in CVPR, 2022, pp. 6543–6552.
more data-driven, end-to-end architecture, and extending the                                [24] L. Rowe et al., “FJMP: Factorized joint multi-agent motion prediction
                                                                                                 over learned directed acyclic interaction graphs,” in CVPR, 2023, pp.
framework to real-world applications.                                                            13 745–13 755.
                                                                                            [25] L. Zhang et al., “Simpl: A simple and efficient multi-agent motion
                                  R EFERENCES                                                    prediction baseline for autonomous driving,” IEEE Robot. Autom. Lett.,
                                                                                                 2024.
 [1] W. Ding et al., “Epsilon: An efficient planning system for automated                   [26] A. G. Cunningham et al., “MPDM: Multipolicy decision-making in
     vehicles in highly interactive environments,” IEEE Trans. on Robot.,                        dynamic, uncertain environments for autonomous driving,” in ICRA.
     vol. 38, no. 2, pp. 1118–1138, 2021.                                                        IEEE, 2015, pp. 1670–1677.
 [2] H. Cui et al., “Multimodal trajectory predictions for autonomous                       [27] L. Zhang et al., “Efficient uncertainty-aware decision-making for
     driving using deep convolutional networks,” in ICRA. IEEE, 2019.                            automated driving using guided branching,” in ICRA. IEEE, 2020,
 [3] C. Hubmann et al., “Automated driving in uncertain environments:                            pp. 3291–3297.
     Planning with interaction and uncertain maneuver prediction,” IEEE                     [28] J. Hardy and M. Campbell, “Contingency planning over probabilistic
     Trans. on Intel. Veh., vol. 3, no. 1, pp. 5–17, 2018.                                       obstacle predictions for autonomous road vehicles,” IEEE Trans. on
 [4] W. Zeng et al., “DSDNet: Deep structured self-driving network,” in                          Robot., vol. 29, no. 4, pp. 913–929, 2013.
     ECCV. Springer. Springer, 2020, pp. 156–172.                                           [29] Y. Chen et al., “Interactive joint planning for autonomous vehicles,”
 [5] A. Cui et al., “Lookout: Diverse multi-future prediction and planning                       IEEE Robot. Autom. Lett., 2023.
     for self-driving,” in ICCV. IEEE, 2021, pp. 16 107–16 116.                             [30] R. Wang et al., “Interaction-aware model predictive control for au-
 [6] S. Casas et al., “MP3: A unified model to map, perceive, predict and                        tonomous driving,” in ECC. IEEE, pp. 1–6.
     plan,” in CVPR, 2021, pp. 14 403–14 412.                                               [31] D. Mayne, “A second-order gradient method for determining optimal
 [7] N. Rhinehart et al., “PRECOG: Prediction conditioned on goals in                            trajectories of non-linear discrete-time systems,” Intl. J. Ctrl., vol. 3,
     visual multi-agent settings,” in ICCV. IEEE, 2019, pp. 2821–2830.                           no. 1, pp. 85–95, 1966.
 [8] H. Song et al., “PiP: Planning-informed trajectory prediction for                      [32] B. Wilson et al., “Argoverse 2: Next generation datasets for self-
     autonomous driving,” in ECCV. Springer. Springer, 2020, pp. 598–                            driving perception and forecasting,” arXiv preprint arXiv:2301.00493,
     614.                                                                                        2023.
 [9] T. Salzmann et al., “Trajectron++: Dynamically-feasible trajectory
     forecasting with heterogeneous data,” in ECCV. Springer, 2020, pp.
     683–700.


                                                                                       13681
          Authorized licensed use limited to: Anhui University. Downloaded on December 31,2025 at 09:17:11 UTC from IEEE Xplore. Restrictions apply.
